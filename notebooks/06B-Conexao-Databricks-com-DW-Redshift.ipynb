{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "22279b03-27e3-4b41-b464-82a9e0c8e189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura das credenciais de forma segura pelo Spark ðŸ”\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/caio.santos.cavalheiro@gmail.com/00-Configuracao\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4591079f-a68f-4808-86a1-90f7cca8c61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Œ ConexÃ£o do Redshift com o Databricks\n",
    "### âš ï¸ **ObservaÃ§Ã£o**\n",
    "\n",
    "> Mesmo parecendo redundante analisar os dados no Databricks depois de salvar no Redshift, essa etapa foi feita **com foco no aprendizado**.\n",
    "\n",
    "> O objetivo aqui foi praticar dois pontos importantes:\n",
    "> - Como **salvar DataFrames do Databricks no Redshift**\n",
    "> - Como **ler tabelas diretamente do Redshift via JDBC no Databricks**\n",
    "\n",
    "> Isso reforÃ§a o entendimento do fluxo completo entre o Data Lake, Data Warehouse e ferramentas de anÃ¡lise.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b7f3817-0558-4c8c-8762-bd1adad917ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ”¶ ConfiguraÃ§Ãµes de Acesso\n",
    "\n",
    "Nesse trecho definimos alguns parÃ¢metros necessÃ¡rios para conectarmos com o Redshift.\n",
    "\n",
    "ApÃ³s a configuraÃ§Ã£o, vamos fazer a leitura da tabela do Redshift usando o Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e95b132d-52ff-4559-a629-363075bba113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”¹ Define URL JDBC e credenciais para conectar ao Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a60362-bfa4-4920-b9de-33a0e0af8376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ConexÃ£o realizada com sucesso!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ConexÃ£o bem-sucedida! Arquivos encontrados na camada Bronze e Silver:\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraÃ§Ãµes de conexÃ£o\n",
    "jdbc_url = \"jdbc:redshift://workgroup-brasileirao.296062575004.us-east-2.redshift-serverless.amazonaws.com:5439/dev\"\n",
    "db_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": credentials_dict['redshift_password'],\n",
    "    \"driver\": \"com.amazon.redshift.jdbc42.Driver\"\n",
    "}\n",
    "print('âœ… ConexÃ£o realizada com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49376f1b-10a8-4d18-8029-eb76a97237a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’  mart_desempenho_clubes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94e22ed4-2cb8-4e8a-ab1a-44778307a4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ”¹ LÃª a tabela 'mart_desempenho_clubes' diretamente do Data Warehouse - Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8ecdd1-a9c5-4f56-827d-20d58518a783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Leitura \"df_desemp_clubes\" feita com sucesso !\n"
     ]
    }
   ],
   "source": [
    "# Teste de leitura de uma tabela\n",
    "df_desemp_clubes = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.mart_desempenho_clubes\",  # esse Ã© o segredo!\n",
    "    properties=db_properties\n",
    ")\n",
    "print('âœ… Leitura \"df_desemp_clubes\" feita com sucesso !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36512df6-f093-4db8-8371-cc8d14750332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’  mart_desempenho_jogadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee706474-591e-4ddc-93a6-5e892de83ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ”¹ LÃª a tabela 'mart_desempenho_clubes' diretamente do Data Warehouse - Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26acb8e-8746-4450-bced-11482afdbd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Leitura \"df_desemp_jogadores\" feita com sucesso !\n"
     ]
    }
   ],
   "source": [
    "# Teste de leitura de uma tabela\n",
    "df_desemp_jogadores = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.mart_desempenho_jogadores\",  # esse Ã© o segredo!\n",
    "    properties=db_properties\n",
    ")\n",
    "print('âœ… Leitura \"df_desemp_jogadores\" feita com sucesso !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d6aff-d35b-4960-a8b6-e139590f1405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’  mart_classificacao_tier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3500b3d2-d580-4c8c-8a7b-b7d28d18c91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ”¹ LÃª a tabela 'mart_desempenho_clubes' diretamente do Data Warehouse - Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eade65ed-a5fe-4080-9c72-26d347a3bdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Leitura \"df_class_tier\" feita com sucesso !\n"
     ]
    }
   ],
   "source": [
    "# Teste de leitura de uma tabela\n",
    "df_class_tier = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.mart_classificacao_tier\",  # esse Ã© o segredo!\n",
    "    properties=db_properties\n",
    ")\n",
    "print('âœ… Leitura \"df_class_tier\" feita com sucesso !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294c5777-123d-4ec2-b6c7-1a8577d529f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’  mart_todas_partidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd749672-beb3-416b-ab28-cf636f98d5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ”¹ LÃª a tabela 'mart_desempenho_clubes' diretamente do Data Warehouse - Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d106e9c-60f5-4f8a-aaaf-5fef63d6e884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Leitura \"df_todas_partidas\" feita com sucesso !\n"
     ]
    }
   ],
   "source": [
    "# Teste de leitura de uma tabela\n",
    "df_todas_partidas = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.mart_todas_partidas\",  # esse Ã© o segredo!\n",
    "    properties=db_properties\n",
    ")\n",
    "print('âœ… Leitura \"df_todas_partidas\" feita com sucesso !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763922ec-8732-4194-9127-7c43dc74254a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ’  mart_info_jogadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb6587fc-93fd-4005-8be8-3c48d26ea3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ”¹ LÃª a tabela 'mart_desempenho_clubes' diretamente do Data Warehouse - Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d98637-910f-4e7c-9221-37841e06c0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Leitura \"df_info_jogadores\" feita com sucesso !\n"
     ]
    }
   ],
   "source": [
    "# Teste de leitura de uma tabela\n",
    "df_info_jogadores = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"public.mart_info_jogadores\",  # esse Ã© o segredo!\n",
    "    properties=db_properties\n",
    ")\n",
    "print('âœ… Leitura \"df_info_jogadores\" feita com sucesso !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bb733b-935e-4def-a340-90270a679072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Œ Criando o Database e Tables a partir dos DataFrames\n",
    "\n",
    "Antes de usar SQL no Databricks, Ã© necessÃ¡rio **ler as tabelas do Redshift com Spark** e transformÃ¡-las em **Tables**.\n",
    "\n",
    "Isso acontece porque o SQL do Databricks sÃ³ reconhece:\n",
    "- Tabelas registradas no metastore (Hive)\n",
    "- Ou **Tables criadas via Spark**\n",
    "\n",
    "Por isso, primeiro carregamos as tabelas com `spark.read.jdbc()` e, em seguida, registramos cada uma como Tables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7469f5-6802-420a-bfdb-e7a871e2c606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”¹ CriaÃ§Ã£o do Database e fazendo uso dele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c53b4ed-a4b4-49f6-b8cd-c4a84651136b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS marts_mvp_brasileirao\")\n",
    "spark.sql(\"USE marts_mvp_brasileirao\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b26a300-065c-4fea-9351-060facd8e921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”¹ CriaÃ§Ã£o das TempViews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781ab65e-ff6d-46b4-b441-e8d57c946c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_desemp_clubes.write.mode(\"overwrite\").saveAsTable(\"marts_mvp_brasileirao.desempenho_clubes\")\n",
    "df_desemp_jogadores.write.mode(\"overwrite\").saveAsTable(\"marts_mvp_brasileirao.desempenho_jogadores\")\n",
    "df_class_tier.write.mode(\"overwrite\").saveAsTable(\"marts_mvp_brasileirao.classificacao_tier\")\n",
    "df_todas_partidas.write.mode(\"overwrite\").saveAsTable(\"marts_mvp_brasileirao.todas_partidas\")\n",
    "df_info_jogadores.write.mode(\"overwrite\").saveAsTable(\"marts_mvp_brasileirao.informacao_jogadores\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2992466188182743,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06B-Conexao-Databricks-com-DW-Redshift",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}