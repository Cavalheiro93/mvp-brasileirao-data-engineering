# âš½ MVP - Engenharia de Dados com Databricks, AWS e Redshift

Este projeto foi desenvolvido como parte da PÃ³s-GraduaÃ§Ã£o em Data Science e Analytics da PUC-Rio.  
O objetivo Ã© construir um pipeline completo de Engenharia de Dados usando ferramentas do mercado, com foco em dados reais do Campeonato Brasileiro SÃ©rie A 2024.


<br></br>
## ğŸ“š SumÃ¡rio

- [ğŸš€ Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [ğŸ“ Estrutura de Pastas](#-estrutura-de-pastas)
- [ğŸ“’ Notebooks do Projeto](#-notebooks-do-projeto)
- [ğŸ§­ Etapas do Projeto](#-etapas-do-projeto)


<br></br>
## ğŸš€ Tecnologias Utilizadas

- **Databricks Community Edition**: para ingestÃ£o, transformaÃ§Ã£o e tratamento dos dados com PySpark (ETL)
- **AWS S3**: armazenamento em Data Lake (Bronze, Silver, Gold)
- **Amazon Redshift Serverless**: utilizado como Data Warehouse para armazenar a camada Gold do projeto, estruturada em Data Marts temÃ¡ticos focados em clubes, jogadores e mÃ©tricas estatÃ­sticas.
- **PySpark + SQL**: para manipulaÃ§Ã£o, modelagem e criaÃ§Ã£o de data marts
- **GitHub**: versionamento e documentaÃ§Ã£o do projeto


<br></br>
## ğŸ“ Estrutura de Pastas

```plaintext
â”œâ”€â”€ ğŸ“ data/                â†’ ContÃ©m os dados divididos por camadas do Data Lake
â”‚   â”œâ”€â”€ ğŸ“‚ bronze/          â†’ Dados brutos, exatamente como foram recebidos (raw)
â”‚   â”œâ”€â”€ ğŸ“‚ silver/          â†’ Dados tratados, limpos e padronizados
â”‚   â””â”€â”€ ğŸ“‚ gold/            â†’ Dados modelados prontos para anÃ¡lise e consumo (Data Marts)

â”œâ”€â”€ ğŸ“ notebooks/           â†’ Notebooks com cada etapa do pipeline de dados
â”‚   â””â”€â”€ 00-Configuracao     â†’ Notebook de configuraÃ§Ã£o de ambiente e credenciais

â”œâ”€â”€ ğŸ“ images/              â†’ Diagramas, capturas de tela e elementos visuais do projeto
â”œâ”€â”€ README.md              â†’ DocumentaÃ§Ã£o geral do projeto
â””â”€â”€ placeholder.txt        â†’ Arquivo temporÃ¡rio para inicializar o repositÃ³rio
```


<br></br>
## ğŸ“’ Notebooks do Projeto

| Ordem | Nome do Notebook         | DescriÃ§Ã£o                                                       | Link
|-------|--------------------------|-----------------------------------------------------------------|-------|
| 00    | `00-Configuracao.ipynb`  | Leitura das credenciais, configuraÃ§Ã£o do S3 e testes de conexÃ£o |[ğŸ”—](notebooks/00-Configuracao.ipynb)


### ğŸ“Œ ObservaÃ§Ãµes
> ğŸ” **Credenciais AWS** nÃ£o estÃ£o incluÃ­das no repositÃ³rio por seguranÃ§a.  
> O arquivo `aws_credentials.json` foi usado localmente para leitura via Spark no notebook `00-Configuracao`.

---

<br></br>
# ğŸ§­ Etapas do Projeto


## 1. ConfiguraÃ§Ã£o Inicial ğŸ› ï¸
O projeto foi construÃ­do com base na integraÃ§Ã£o entre `AWS` e `Databricks`, com o objetivo de aprofundar o uso dessas duas tecnologias.
<br>
Durante o processo de conexÃ£o do Databricks ao AWS S3, percebi que nÃ£o seria seguro deixar as chaves de acesso expostas no cÃ³digo, jÃ¡ que qualquer usuÃ¡rio poderia visualizÃ¡-las.
<br>
Pensando em boas prÃ¡ticas e seguranÃ§a, esta etapa inicial foi dedicada Ã  configuraÃ§Ã£o do ambiente, incluindo a criaÃ§Ã£o de um arquivo
`aws_credentials.json`, que permite a conexÃ£o com a AWS de forma segura e controlada.


### 1.A - ConfiguraÃ§Ã£o das chaves de acesso | [00-Configuracao ğŸ“](notebooks/00-Configuracao.ipynb)
- CriaÃ§Ã£o da conta na AWS
- CriaÃ§Ã£o do bucket S3 com as camadas bronze, silver e gold
- CriaÃ§Ã£o de credenciais IAM e geraÃ§Ã£o das chaves de acesso
- Armazenamento seguro das credenciais em um arquivo `.json`
- ConfiguraÃ§Ã£o do acesso ao S3 no Databricks usando `spark.conf`

<br></br>

### 2. ğŸ¥‰ CatÃ¡logo da Camada Bronze | [01-Catalogo de Dados no Metastore do Databricks BronzeğŸ“](notebooks/01-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Bronze.ipynb)  

Esta etapa teve como objetivo principal visualizar e documentar todas as colunas disponÃ­veis nos arquivos brutos, a fim de entender a estrutura dos dados recebidos e decidir quais colunas seriam aproveitadas nas prÃ³ximas etapas do pipeline.

- CriaÃ§Ã£o do catÃ¡logo no Metastore do Databricks com os dados brutos (raw) armazenados na camada Bronze
- Registro do nome dos campos, tipo e respectivas descriÃ§Ãµes de cada um deles
- AdiÃ§Ã£o de atributos informativos: valores mÃ­nimos e mÃ¡ximos, total de registros, registros nulos e registros distintos
- Detalhamento da fonte dos dados: link da origem, nome do arquivo original e nome utilizado no Databricks/S3

<br></br>

### 3. ğŸ“™ Limpeza e Tratamento dos Dados


#### 3A. IngestÃ£o Bronze - Todas as Partidas | [02A-Ingestao-Bronze-TodasPartidasğŸ“](notebooks/02A-Ingestao-Bronze-TodasPartidas.ipynb)  
Este notebook realiza a leitura dos dados de partidas diretamente da camada Bronze, aplicando diversas transformaÃ§Ãµes para preparÃ¡-los para a camada Silver.
- Leitura do arquivo `BrasilSerieA_2024_TodasPartidas.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes (ex: colunas com odds de apostas)
- Filtro para considerar apenas partidas da temporada **2024**
- RemoÃ§Ã£o de linhas duplicadas e com dados ausentes crÃ­ticos
- CriaÃ§Ã£o das colunas:
  - `Match_ID`: identificador Ãºnico da partida
  - `Trimestre`: com base na data da partida
  - `Turno`: para indicar se Ã© 1Âº ou 2Âº turno do campeonato
- Salvamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3B. IngestÃ£o Bronze - ClassificaÃ§Ã£o Final | [02B-Ingestao-Bronze-ClassificacaoğŸ“](notebooks/02B-Ingestao-Bronze-Classificacao.ipynb)

Este notebook realiza o tratamento da tabela de classificaÃ§Ã£o dos clubes, transformando os dados brutos da Bronze em um formato estruturado para anÃ¡lise na camada Silver.

- Leitura do arquivo `BrasilSerieA_2024_ClassificacaoFinal.csv` na Bronze
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- Salvamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3C. IngestÃ£o Bronze - EstatÃ­sticas por Jogador e Partida | [02C-Ingestao-Bronze-EstatisticaJogadorPorPartidağŸ“](notebooks/02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb)

Este notebook trata os dados estatÃ­sticos dos jogadores por partida, realizando ajustes essenciais antes de armazenÃ¡-los na camada Silver.

- Leitura do arquivo `BrasilSerieA_2024_EstatisticaJogadorPorPartida.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes ou redundantes
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- CorreÃ§Ã£o no Tipo de Dado de algumas colunas
- âš ï¸ Ajustes na base de dados por falta de informaÃ§Ã£o
- Salvamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3D. CorreÃ§Ã£o de Datas e Partidas Ausentes | [02D-Correcao-Datas-e-Partidas-AusentesğŸ“](notebooks/02D-Correcao-Datas-e-Partidas-Ausentes.ipynb)

Este notebook realiza correÃ§Ãµes importantes nos dados da tabela de estatÃ­sticas dos jogadores, garantindo integridade e consistÃªncia antes de seguirmos para anÃ¡lises na camada Silver.

- IdentificaÃ§Ã£o de divergÃªncias entre datas de partidas nos arquivos de estatÃ­sticas e todas as partidas
- Encontrado um padrÃ£o, onde as datas diferentes das partidas sÃ£o de -1 dia
- CorreÃ§Ã£o das datas incorretas com base no dataset validado (fbref.com)
- Salvamento do DataFrame final corrigido na camada Silver no formato Parquet

<br></br>

### 4. ğŸ—‚ï¸ CatÃ¡logo da Camada Silver | [03-Catalogo de Dados no Metastore do Databricks SilverğŸ“](notebooks/03-Catalogo%de%Dados.ipynb)

Este notebook Ã© responsÃ¡vel por registrar no Metastore as tabelas jÃ¡ tratadas da camada Silver, possibilitando o consumo via SQL e outras ferramentas.

- CriaÃ§Ã£o do Database especÃ­fico para os dados tratados (camada Silver)
- ConversÃ£o dos arquivos tratados de .parquet para o formato Delta
- Registro das tabelas da camada Silver no Metastore com o caminho no S3
- Tabelas disponÃ­veis para consulta direta com spark.sql("SELECT * FROM ...")


### 5. ğŸ¥‡ TransformaÃ§Ã£o para a Camada Gold - Mart de Clubes | 04A-Transformacao-Gold-Mart-Clubes

Nessa etapa damos inÃ­cio Ã  construÃ§Ã£o do **Data Warehouse** do projeto, criando nossas tabelas da **camada Gold**.
O foco aqui Ã© consolidar os dados tratados na camada Silver em uma estrutura otimizada para consumo analÃ­tico, utilizando mÃ©tricas agregadas, normalizaÃ§Ãµes e criaÃ§Ã£o de scores personalizados.

Principais etapas desenvolvidas:
...

