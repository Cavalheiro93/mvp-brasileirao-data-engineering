<p align="center">
  <img src="images/icons/spfc.jpg" width="75" />
  <img src="images/icons/brasileirao-2024.jpeg" width="134" />
  <img src="images/icons/spfc.jpg" width="75" />
  <img src="images/icons/brasileirao-2024.jpeg" width="134" />
  <img src="images/icons/spfc.jpg" width="75" />
  <img src="images/icons/brasileirao-2024.jpeg" width="134" />
  <img src="images/icons/spfc.jpg" width="75" />
</p>

# MVP - Engenharia de Dados | AnÃ¡lise do Desempenho do SÃ£o Paulo no BrasileirÃ£o 2024 e IdentificaÃ§Ã£o de possÃ­veis ReforÃ§os

## ğŸ¯ Objetivo do Projeto
Este projeto tem como objetivo construir um pipeline completo de Engenharia de Dados usando ferramentas amplamente utilizadas no mercado, como AWS S3, Databricks e Amazon Redshift, com foco em dados reais do Campeonato Brasileiro 2024.

A proposta Ã© resolver, de forma estruturada, o seguinte problema de negÃ³cio:

> ### "Quais sÃ£o os principais pontos fortes e fracos do desempenho do SÃ£o Paulo FC ao longo da temporada, e quais reforÃ§os estatisticamente fariam sentido para a prÃ³xima temporada?"

Para isso, o projeto foi dividido em etapas claras, que envolvem:

- A coleta de dados pÃºblicos no Kaggle;
- A organizaÃ§Ã£o dos dados em um Data Lake (camadas Bronze, Silver e Gold);
- A modelagem de Data Marts por clube e por jogador;
- O carregamento para um Data Warehouse na nuvem (Amazon Redshift);
- E a anÃ¡lise final com base nas perguntas de negÃ³cio definidas no inÃ­cio.

AlÃ©m de responder Ã s perguntas, o projeto busca aplicar boas prÃ¡ticas de organizaÃ§Ã£o, versionamento, documentaÃ§Ã£o e uso de ferramentas modernas em um fluxo completo de ETL com PySpark, entregando um produto final reutilizÃ¡vel, didÃ¡tico e voltado para portfÃ³lio profissional.

<br></br>
## ğŸ“š SumÃ¡rio

- [ğŸš€ Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [ğŸ”„ Pipeline do Projeto](#-pipeline-do-projeto)
- [ğŸ“ Estrutura de Pastas](#-estrutura-de-pastas)
- [ğŸ“’ Notebooks do Projeto](#-notebooks-do-projeto)
- [ğŸ§­ Etapas do Projeto](#-etapas-do-projeto)
- [ğŸ“Š Perguntas e Respostas do Projeto](#-perguntas-e-respostas-do-projeto)


<br></br>


## ğŸš€ Tecnologias Utilizadas

- **Databricks Community Edition**: para ingestÃ£o, transformaÃ§Ã£o e tratamento dos dados com PySpark (ETL)
- **AWS S3**: armazenamento em Data Lake (Bronze, Silver, Gold)
- **Amazon Redshift Serverless**: utilizado como Data Warehouse para armazenar a camada Gold do projeto, estruturada em Data Marts temÃ¡ticos focados em clubes, jogadores e mÃ©tricas estatÃ­sticas.
- **PySpark + SQL**: para manipulaÃ§Ã£o, modelagem e criaÃ§Ã£o de data marts
- **GitHub**: versionamento e documentaÃ§Ã£o do projeto


<br></br>


## ğŸ”„ Pipeline do Projeto
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/pipeline/pipeline-dados.jpg)


<br></br>


## ğŸ“ Estrutura de Pastas

```plaintext
â”œâ”€â”€ ğŸ“ data/                â†’ ContÃ©m os dados divididos por camadas do Data Lake
â”‚   â”œâ”€â”€ ğŸ“‚ bronze/          â†’ Dados brutos, exatamente como foram recebidos (raw)
â”‚   â”œâ”€â”€ ğŸ“‚ silver/          â†’ Dados tratados, limpos e padronizados
â”‚   â””â”€â”€ ğŸ“‚ gold/            â†’ Dados modelados prontos para anÃ¡lise e consumo (Data Marts)
|
â”œâ”€â”€ ğŸ“ notebooks/           â†’ Notebooks com cada etapa do pipeline de dados
|
â”œâ”€â”€ ğŸ“ images/              â†’ Diagramas, capturas de tela e elementos visuais do projeto
â”‚   â”œâ”€â”€ ğŸ“‚ AWS-Redshift/    â†’ Imagens referente ao Redshift, para auxiliar no entendimento do processo
â”‚   â”œâ”€â”€ ğŸ“‚ AWS-S3/          â†’ Imagens referente ao S3, para auxiliar no entendimento do processo
â”‚   â”œâ”€â”€ ğŸ“‚ icons/           â†’ Icones personalizados para o README do GitHub
â”‚   â”œâ”€â”€ ğŸ“‚ notebook-images/ â†’ Imagens que deverÃ£o ser anexadas nos notebooks
â”‚   â”œâ”€â”€ ğŸ“‚ pipeline/        â†’ Imagem do Pipeline do Projeto
|
â”œâ”€â”€ README.md              â†’ DocumentaÃ§Ã£o geral do projeto
â””â”€â”€ placeholder.txt        â†’ Arquivo temporÃ¡rio para inicializar o repositÃ³rio
```


<br></br>
## ğŸ“’ Notebooks do Projeto


| Ordem | Nome do Notebook | DescriÃ§Ã£o | Link |
|-------|------------------|-----------|------|
| 00 | `00-Configuracao.ipynb` | Leitura das credenciais, configuraÃ§Ã£o do S3 e testes de conexÃ£o | [ğŸ”—](notebooks/00-Configuracao.ipynb) |
| 01 | `01-Catalogo de Dados no Metastore do Databricks Bronze.ipynb` | CriaÃ§Ã£o do catÃ¡logo no Metastore com os dados brutos da camada Bronze | [ğŸ”—](notebooks/01-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Bronze.ipynb) |
| 02A | `02A-Ingestao-Bronze-TodasPartidas.ipynb` | Leitura e tratamento inicial do arquivo de partidas (ajuste de datas, nomes e tipos) | [ğŸ”—](notebooks/02A-Ingestao-Bronze-TodasPartidas.ipynb) |
| 02B | `02B-Ingestao-Bronze-Classificacao.ipynb` | Leitura da classificaÃ§Ã£o final dos clubes, ajustes e criaÃ§Ã£o da tabela na camada Silver | [ğŸ”—](notebooks/02B-Ingestao-Bronze-Classificacao.ipynb) |
| 02C | `02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb` | Leitura e tratamento das estatÃ­sticas por jogador; limpeza e padronizaÃ§Ã£o de colunas | [ğŸ”—](notebooks/02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb) |
| 02D | `02D-Correcao-Datas-e-Partidas-Ausentes.ipynb` | CorreÃ§Ãµes manuais em datas e partidas ausentes nas estatÃ­sticas do campeonato | [ğŸ”—](notebooks/02D-Correcao-Datas-e-Partidas-Ausentes.ipynb) |
| 03 | `03-Catalogo de Dados no Metastore do Databricks Silver.ipynb` | Registro das tabelas tratadas da camada Silver no Metastore (com caminho S3) | [ğŸ”—](notebooks/03-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Silver.ipynb) |
| 04A | `04A-Transformacao-Gold-Mart-Clubes.ipynb` | AgregaÃ§Ãµes, normalizaÃ§Ãµes e criaÃ§Ã£o do Mart com desempenho dos clubes na camada Gold | [ğŸ”—](notebooks/04A-Transformacao-Gold-Mart-Clubes.ipynb) |
| 04B | `04B-Transformacao-Gold-Mart-Jogadores.ipynb` | CriaÃ§Ã£o do Mart com desempenho individual dos jogadores e informaÃ§Ãµes complementares | [ğŸ”—](notebooks/04B-Transformacao-Gold-Mart-Jogadores.ipynb) |
| 04C | `04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb` | ConsolidaÃ§Ã£o das funÃ§Ãµes e posiÃ§Ãµes dos jogadores, jogos por funÃ§Ã£o e versatilidade tÃ¡tica | [ğŸ”—](notebooks/04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb) |
| 05 | `05-Catalogo de Dados no Metastore do Databricks Gold.ipynb` | Registro das tabelas tratadas da camada Gold no Metastore (com caminho S3) | [ğŸ”—](notebooks/05-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Gold.ipynb) |
| 06A | `06A-Carregamento-DW-Redshift-com-Databricks.ipynb` | Envio das tabelas da camada Gold do S3 para o Redshift via JDBC | [ğŸ”—](notebooks/06A-Carregamento-DW-Redshift-com-Databricks.ipynb) |
| 06B | `06B-Conexao-Databricks-com-DW-Redshift.ipynb` | Leitura das tabelas do Redshift diretamente no Databricks usando conexÃ£o JDBC | [ğŸ”—](notebooks/06B-Conexao-Databricks-com-DW-Redshift.ipynb) |
| 06C | `06C-Respostas.ipynb` | Respostas analÃ­ticas Ã s perguntas do projeto, com consultas SQL e visualizaÃ§Ãµes em Matplotlib | [ğŸ”—](notebooks/06C-Respostas.ipynb) |




### ğŸ“Œ ObservaÃ§Ãµes
> ğŸ” **Credenciais AWS** nÃ£o estÃ£o incluÃ­das no repositÃ³rio por seguranÃ§a.  
> O arquivo `aws_credentials.json` foi usado localmente para leitura via Spark no notebook `00-Configuracao`.

---


<br></br>


# ğŸ§­ Etapas do Projeto


## 1. ConfiguraÃ§Ã£o Inicial ğŸ› ï¸
O projeto foi construÃ­do com base na integraÃ§Ã£o entre `AWS` e `Databricks`, com o objetivo de aprofundar o uso dessas duas tecnologias.
<br>
Durante o processo de conexÃ£o do Databricks ao AWS S3, percebi que nÃ£o seria seguro deixar as chaves de acesso expostas no cÃ³digo, jÃ¡ que qualquer usuÃ¡rio poderia visualizÃ¡-las.
<br>
Pensando em boas prÃ¡ticas e seguranÃ§a, esta etapa inicial foi dedicada Ã  configuraÃ§Ã£o do ambiente, incluindo a criaÃ§Ã£o de um arquivo
`aws_credentials.json`, que permite a conexÃ£o com a AWS de forma segura e controlada.

#### 1.A - ConfiguraÃ§Ã£o das chaves de acesso | [00-Configuracao ğŸ“](notebooks/00-Configuracao.ipynb)
- CriaÃ§Ã£o da conta na AWS
- CriaÃ§Ã£o do bucket S3 com as camadas bronze, silver e gold
- CriaÃ§Ã£o de credenciais IAM e geraÃ§Ã£o das chaves de acesso
- Armazenamento seguro das credenciais em um arquivo `.json`
- ConfiguraÃ§Ã£o do acesso ao S3 no Databricks usando `spark.conf`


<br></br>


## 2. ExtraÃ§Ã£o dos Dados e CatalogaÃ§Ã£o Inicial da camada Bronze ğŸ“¦ğŸ¥‰
Nesta etapa, realizamos a extraÃ§Ã£o dos dados brutos (raw) da fonte original (`Kaggle`), com armazenamento direto na camada Bronze do nosso Data Lake no S3 (`mvp-brasileirao-2024`).
<br>
Em seguida, iniciamos o processo de catalogaÃ§Ã£o para entender a estrutura dos dados recebidos e decidir quais colunas seriam aproveitadas nas prÃ³ximas etapas do pipeline.

#### 2.A - ExtraÃ§Ã£o dos Dados e armazenamento no AWS S3 |  [Arquivos da camada Bronze](https://github.com/Cavalheiro93/mvp-brasileirao-data-engineering/tree/main/data/bronze)
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-bronze-arquivos-raw.jpg)

#### 2.B  CatÃ¡logo da Camada Bronze | [01-Catalogo de Dados no Metastore do Databricks BronzeğŸ“](notebooks/01-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Bronze.ipynb)  
- CriaÃ§Ã£o do catÃ¡logo no Metastore do Databricks com os dados brutos (raw) armazenados na camada Bronze
- Registro do nome dos campos, tipo e respectivas descriÃ§Ãµes de cada um deles
- AdiÃ§Ã£o de atributos informativos: valores mÃ­nimos e mÃ¡ximos, total de registros, registros nulos e registros distintos
- Detalhamento da fonte dos dados: link da origem, nome do arquivo original e nome utilizado no Databricks/S3


<br></br>


## 3. Processamento da Camada Silver: Limpeza, TransformaÃ§Ã£o e CatalogaÃ§Ã£o ğŸ§¹ğŸ¥ˆ
ApÃ³s a extraÃ§Ã£o e armazenamento dos dados brutos na camada Bronze, esta etapa Ã© dedicada Ã  preparaÃ§Ã£o dos dados para consumo analÃ­tico.
<br>
Realizamos a limpeza, padronizaÃ§Ã£o e transformaÃ§Ã£o de cada uma das tabelas originais, tratando problemas como colunas irrelevantes, formataÃ§Ãµes inconsistentes e divergÃªncias nas datas das partidas.
<br>
Ao final do processo, os dados tratados sÃ£o armazenados na camada Silver do Data Lake no formato Delta e Parquet e, em seguida, registrados no Metastore do Databricks com o caminho S3, possibilitando consultas diretas via SQL.


#### 3.A - TransformaÃ§Ã£o dos Dados e armazenamento no AWS S3 (Parquet e Delta) | [Arquivos da camada Silver](https://github.com/Cavalheiro93/mvp-brasileirao-data-engineering/tree/main/data/silver)
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-silver-pastas-parquet-delta.jpg)


#### 3B. IngestÃ£o Bronze - Todas as Partidas | [02A-Ingestao-Bronze-TodasPartidasğŸ“](notebooks/02A-Ingestao-Bronze-TodasPartidas.ipynb)  
- Leitura do arquivo `BrasilSerieA_2024_TodasPartidas.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes (ex: colunas com odds de apostas)
- Filtragem da temporada **2024** apenas.
- RemoÃ§Ã£o de linhas duplicadas e com dados ausentes crÃ­ticos
- CriaÃ§Ã£o das colunas:
  - `Match_ID`: identificador Ãºnico da partida
  - `Trimestre`: com base na data da partida
  - `Turno`: para indicar se Ã© 1Âº ou 2Âº turno do campeonato
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3C. IngestÃ£o Bronze - ClassificaÃ§Ã£o Final | [02B-Ingestao-Bronze-ClassificacaoğŸ“](notebooks/02B-Ingestao-Bronze-Classificacao.ipynb)
- Leitura do arquivo `BrasilSerieA_2024_ClassificacaoFinal.csv` na Bronze
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3D. IngestÃ£o Bronze - EstatÃ­sticas por Jogador e Partida | [02C-Ingestao-Bronze-EstatisticaJogadorPorPartidağŸ“](notebooks/02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb)
- Leitura do arquivo `BrasilSerieA_2024_EstatisticaJogadorPorPartida.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes ou redundantes
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- CorreÃ§Ã£o no Tipo de Dado de algumas colunas
- âš ï¸ Idade do Jogador 'Felipe Vieira' (Fluminense) estava vazia, e portanto foi ajustada manualmente baseando-se no site https://www.ogol.com.br/jogador/felipe-andrade/620537?epoca_id=153
- âš ï¸ Input manual das Partidas que estavam pendentes no arquivo original:
  - Fluminense x GrÃªmio (01/11/2024), informaÃ§Ãµes baseada no site: https://fbref.com/pt/partidas/bb1a4828/Fluminense-Gremio-2024Novembro1-Serie-A
  - AtlÃ©tico-MG x Athletico-PR (07/12/2024), informaÃ§Ãµes baseada no site: https://fbref.com/pt/partidas/fd4de3a4/Atletico-Mineiro-Athletico-Paranaense-2024Dezembro8-Serie-A
  - Input foi feito atravÃ©s do arquivo `BrasilSerieA_2024_EstatisticaJogador_InputManual.csv`
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3E. CorreÃ§Ã£o de Datas e Partidas Ausentes | [02D-Correcao-Datas-e-Partidas-AusentesğŸ“](notebooks/02D-Correcao-Datas-e-Partidas-Ausentes.ipynb)
- IdentificaÃ§Ã£o de divergÃªncias entre datas de partidas nos arquivos de estatÃ­sticas e todas as partidas
- IdentificaÃ§Ã£o do padrÃ£o onde as datas diferentes das partidas sÃ£o de -1 dia
- CorreÃ§Ã£o das datas incorretas com base no dataset validado (fbref.com)
- Armazenamento do DataFrame final corrigido na camada Silver no formato Parquet


#### 3F. CatÃ¡logo da Camada Silver | [03-Catalogo de Dados no Metastore do Databricks SilverğŸ“](notebooks/03-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Silver.ipynb)
- CriaÃ§Ã£o do Database especÃ­fico para os dados tratados (camada Silver)
- ConversÃ£o dos arquivos tratados de .parquet para o formato Delta
- Registro das tabelas da camada Silver no Metastore com o caminho no S3


<br></br>


## 4. TransformaÃ§Ãµes AnalÃ­ticas para Camada Gold e CatÃ¡logo de Dados ğŸ§ªğŸ¥‡
Nesta etapa, consolidamos os dados tratados da camada Silver e criamos as primeiras tabelas analÃ­ticas do nosso **Data Warehouse**.

A Camada Gold serÃ¡ responsÃ¡vel por organizar os dados de forma otimizada para consumo analÃ­tico, atravÃ©s de **Data Marts** modelados por clube e por jogador, alÃ©m de outras tabelas complementares.

O CatÃ¡logo de dados da Camada Gold foram registrados no **Metastore do Databricks** com caminho no S3, permitindo consultas SQL e integraÃ§Ã£o com ferramentas analÃ­ticas.

#### 4A. TransformaÃ§Ãµes AnalÃ­ticas: Enriquecendo os dados da Silver para Gold e armazenamento no AWS S3 | [Arquivos da camada Gold](https://github.com/Cavalheiro93/mvp-brasileirao-data-engineering/tree/main/data/gold)
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-gold-pastas-arquivos-finais.jpg)

#### 4B. TransformaÃ§Ã£o por Clube | [04A-Transformacao-Gold-Mart-Clubes](notebooks/04A-Transformacao-Gold-Mart-Clubes.ipynb)  
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- CriaÃ§Ã£o de Views TemporÃ¡rias
- CriaÃ§Ã£o de mÃ©tricas de normalizaÃ§Ã£o
- Criando pesos para cada uma dessas mÃ©tricas
- Criando um Score beseado na soma dessas mÃ©tricas de normalizaÃ§Ã£o, para o resultado final do desempenho do clube


#### 4C. TransformaÃ§Ã£o por Jogador | [04B-Transformacao-Gold-Mart-Jogadores](notebooks/04B-Transformacao-Gold-Mart-Jogadores.ipynb)
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- CriaÃ§Ã£o de Views TemporÃ¡rias
- CriaÃ§Ã£o de mÃ©tricas de normalizaÃ§Ã£o
- Criando pesos para cada uma dessas mÃ©tricas
- Criando um Score beseado na soma dessas mÃ©tricas de normalizaÃ§Ã£o, para o resultado final do desempenho do Jogador


#### 4D. InformaÃ§Ãµes Complementares dos Jogadores | [04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb](notebooks/04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb)
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- Mapeamento das posiÃ§Ãµes dos Jogadores em cada partida
- Ranqueamento das duas posiÃ§Ãµes mais jogadas por cada jogador
- CriaÃ§Ã£o de colunas de PosiÃ§Ã£o Principal e ImprovisaÃ§Ã£o


#### 4E. CatÃ¡logo da Camada Gold | [05-Catalogo de Dados no Metastore do Databricks Gold](notebooks/05-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Gold.ipynb)
- CriaÃ§Ã£o do Database especÃ­fico para os dados tratados (camada Gold)
- Registro das tabelas da camada Gold no Metastore com o caminho no S3
- AdiÃ§Ã£o de atributos informativos: valores mÃ­nimos e mÃ¡ximos, total de registros, registros nulos e registros distintos
- Detalhamento da fonte dos dados: link da origem, nome do arquivo original e nome utilizado no Databricks/S3


<br></br>


## 5. IntegraÃ§Ã£o com Redshift: Carregamento e ValidaÃ§Ã£o Final ğŸ“¥ <img src="images/icons/redshift.jpg" alt="redshift" width="20" style="vertical-align: middle;"/>
Para consolidar os dados da camada Gold em um ambiente de Data Warehouse, utilizamos o **Amazon Redshift Serverless** integrado ao **Databricks**. 

Abaixo, estÃ¡ resumidamente como foi feita a criaÃ§Ã£o e configuraÃ§Ã£o do ambiente:
### ğŸ”§ CriaÃ§Ã£o do **Workgroup e Namespace** no Redshift Serverless
![Workgroup e Namespace](images/AWS-Redshift/redshift-namespace-workgroup.jpg)

### ğŸŒ HabilitaÃ§Ã£o de Acesso PÃºblico no Redshift Serverless
![Workgroup e Namespace](images/AWS-Redshift/redshift-acesso-publico.jpg)

### ğŸ”“ LiberaÃ§Ã£o da Porta 5439 no Grupo de SeguranÃ§a (VPC)
![Workgroup e Namespace](images/AWS-Redshift/redshift-liberacao-porta-5439.jpg)

### ğŸ“¦ InstalaÃ§Ã£o do **Driver JDBC no Databricks**, com upload do `.jar` e reinicializaÃ§Ã£o do cluster
![Workgroup e Namespace](images/AWS-Redshift/redshift-instalacao-driver-jdbc.jpg)

### ğŸ”— DefiniÃ§Ã£o dos **parÃ¢metros de conexÃ£o**:
  - URL do Redshift Serverless
  - Nome do banco e da tabela
  - UsuÃ¡rio e senha
  - Driver: `com.amazon.redshift.jdbc.Driver`
![Workgroup e Namespace](images/AWS-Redshift/redshift-parametros-conexao.jpg)

### ğŸ” ValidaÃ§Ã£o da carga usando o **Query Editor v2** da AWS
![Workgroup e Namespace](images/AWS-Redshift/redshift-query-editor.jpg)


<br></br>


## 6. IntegraÃ§Ã£o Redshift + Databricks para AnÃ¡lises Finais <img src="images/icons/databricks.jpg" alt="databricks" width="30" style="vertical-align: middle;"/> <img src="images/icons/redshift.jpg" alt="redshift" width="20" style="vertical-align: middle;"/>

Apesar das anÃ¡lises poderem ser feitas diretamente no Redshift, optamos por manter o ambiente de exploraÃ§Ã£o dentro do Databricks, como forma de aprendizado e controle do fluxo de ida e volta dos dados entre as plataformas.

Essa abordagem nos permitiu:
- Consolidar os dados no Redshift como Data Warehouse final
- Aprender como fazer a conexÃ£o JDBC entre Databricks e Redshift
- Validar como fazer a leitura e consulta das tabelas Redshift dentro do Databricks
- Centralizar as anÃ¡lises e respostas no ambiente de notebooks

#### 6.1 Carregamento das Tabelas no Redshift | [06A-Carregamento-DW-Redshift-com-DatabricksğŸ“](notebooks/06A-Carregamento-DW-Redshift-com-Databricks.ipynb)
- Leitura dos arquivos .parquet da camada Gold diretamente do S3
- Escrita das tabelas analÃ­ticas (mart_desempenho_clubes, mart_desempenho_jogadores, mart_info_jogadores) no Redshift via JDBC
- ValidaÃ§Ã£o da carga usando o Query Editor v2

#### 6.2 ConexÃ£o entre Databricks e Redshift | [06B-Conexao-Databricks-com-DW-RedshiftğŸ“](notebooks/06B-Conexao-Databricks-com-DW-Redshift.ipynb)
- Estabelecimento de conexÃ£o JDBC para leitura das tabelas jÃ¡ carregadas no Redshift
- Leitura das tabelas do Redshift dentro do Databricks usando `spark.read.jdbc()`
- ConfirmaÃ§Ã£o da estrutura dos dados e visualizaÃ§Ã£o via PySpark


<br></br>


# ğŸ“Š Perguntas e Respostas do Projeto
## ğŸ““ [`06C-Respostas.ipynb`](notebooks/06C-Respostas.ipynb)

Para finalizar o pipeline, criamos um caderno com as respostas baseadas nas perguntas definidas no inÃ­cio do projeto.

Essa etapa utilizou consultas **SQL com Spark** diretamente sobre as tabelas armazenadas no **Amazon Redshift**, permitindo anÃ¡lises rÃ¡pidas e contextualizadas com base nos dados transformados na Camada Gold.

Algumas respostas foram complementadas com grÃ¡ficos utilizando o matplotlib, trazendo mais clareza e facilitando a contextualizaÃ§Ã£o dos dados analisados.



