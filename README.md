# âš½ MVP - Engenharia de Dados com Databricks, AWS e Redshift

Este projeto foi desenvolvido como parte da PÃ³s-GraduaÃ§Ã£o em Data Science e Analytics da PUC-Rio.  
O objetivo Ã© construir um pipeline completo de Engenharia de Dados usando ferramentas do mercado, com foco em dados reais do Campeonato Brasileiro SÃ©rie A 2024.


<br></br>
## ğŸ“š SumÃ¡rio

- [ğŸš€ Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [ğŸ“ Estrutura de Pastas](#-estrutura-de-pastas)
- [ğŸ“’ Notebooks do Projeto](#-notebooks-do-projeto)
- [ğŸ§­ Etapas do Projeto](#-etapas-do-projeto)


<br></br>
## ğŸš€ Tecnologias Utilizadas

- **Databricks Community Edition**: para ingestÃ£o, transformaÃ§Ã£o e tratamento dos dados com PySpark (ETL)
- **AWS S3**: armazenamento em Data Lake (Bronze, Silver, Gold)
- **Amazon Redshift Serverless**: utilizado como Data Warehouse para armazenar a camada Gold do projeto, estruturada em Data Marts temÃ¡ticos focados em clubes, jogadores e mÃ©tricas estatÃ­sticas.
- **PySpark + SQL**: para manipulaÃ§Ã£o, modelagem e criaÃ§Ã£o de data marts
- **GitHub**: versionamento e documentaÃ§Ã£o do projeto


<br></br>
## ğŸ“ Estrutura de Pastas

```plaintext
â”œâ”€â”€ ğŸ“ data/                â†’ ContÃ©m os dados divididos por camadas do Data Lake
â”‚   â”œâ”€â”€ ğŸ“‚ bronze/          â†’ Dados brutos, exatamente como foram recebidos (raw)
â”‚   â”œâ”€â”€ ğŸ“‚ silver/          â†’ Dados tratados, limpos e padronizados
â”‚   â””â”€â”€ ğŸ“‚ gold/            â†’ Dados modelados prontos para anÃ¡lise e consumo (Data Marts)
|
â”œâ”€â”€ ğŸ“ notebooks/           â†’ Notebooks com cada etapa do pipeline de dados
|
â”œâ”€â”€ ğŸ“ images/              â†’ Diagramas, capturas de tela e elementos visuais do projeto
â”‚   â”œâ”€â”€ ğŸ“‚ icons/           â†’ Icones personalizados para o README do GitHub
â”‚   â”œâ”€â”€ ğŸ“‚ AWS-S3/          â†’ Imagens referente ao S3, para auxiliar no entendimento do processo
|
â”œâ”€â”€ README.md              â†’ DocumentaÃ§Ã£o geral do projeto
â””â”€â”€ placeholder.txt        â†’ Arquivo temporÃ¡rio para inicializar o repositÃ³rio
```


<br></br>
## ğŸ“’ Notebooks do Projeto

| Ordem | Nome do Notebook                                | DescriÃ§Ã£o                                                                                      | Link |
|-------|------------------------------------------       |------------------------------------------------------------------------------------------------|------|
| 00    | `00-Configuracao.ipynb`                                 | Leitura das credenciais, configuraÃ§Ã£o do S3 e testes de conexÃ£o                                | [ğŸ”—](notebooks/00-Configuracao.ipynb) |
| 01    | `01-Catalogo-Bronze.ipynb`                              | CriaÃ§Ã£o do catÃ¡logo no Metastore com os dados brutos da camada Bronze                          | [ğŸ”—](notebooks/01-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Bronze.ipynb) |
| 02A   | `02A-Ingestao-Bronze-TodasPartidas.ipynb`               | Leitura e tratamento inicial do arquivo de partidas (ajuste de datas, nomes e tipos)           | [ğŸ”—](notebooks/02A-Ingestao-Bronze-TodasPartidas.ipynb) |
| 02C   | `02B-Ingestao-Bronze-Classificacao.ipynb`               | Leitura da classificaÃ§Ã£o final dos clubes, ajustes e criaÃ§Ã£o da tabela na camada Silver        | [ğŸ”—](notebooks/02B-Ingestao-Bronze-Classificacao.ipynb) |
| 02B   | `02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb`| Leitura e tratamento das estatÃ­sticas por jogador; limpeza e padronizaÃ§Ã£o de colunas           | [ğŸ”—](notebooks/02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb) |
| 02D   | `02D-Correcao-Datas-e-Partidas-Ausentes.ipynb`          | CorreÃ§Ãµes manuais em datas e partidas ausentes nas estatÃ­sticas do campeonato                  | [ğŸ”—](notebooks/02D-Correcao-Datas-e-Partidas-Ausentes.ipynb) |
| 03    | `03-Catalogo-Silver.ipynb`                              | Registro das tabelas tratadas da camada Silver no Metastore (com caminho S3)                   | [ğŸ”—](notebooks/03-Catalogo-Silver.ipynb) |
| 04A   | `04A-Transformacao-Gold-Mart-Clubes.ipynb`              | AgregaÃ§Ãµes, normalizaÃ§Ãµes e criaÃ§Ã£o do Mart com desempenho dos clubes na camada Gold           | [ğŸ”—](notebooks/04A-Transformacao-Gold-Mart-Clubes.ipynb) |
| 04B   | `04B-Transformacao-Gold-Mart-Jogadores.ipynb`           | CriaÃ§Ã£o do Mart com desempenho individual dos jogadores e informaÃ§Ãµes complementares           | [ğŸ”—](notebooks/04B-Transformacao-Gold-Mart-Jogadores.ipynb) |
| 05    | `05-Carga-Redshift.ipynb`                               | CriaÃ§Ã£o do Namespace e Workgroup no Redshift Serverless, alÃ©m da carga das tabelas Gold        | [ğŸ”—](notebooks/05-Carga-Redshift.ipynb) |



### ğŸ“Œ ObservaÃ§Ãµes
> ğŸ” **Credenciais AWS** nÃ£o estÃ£o incluÃ­das no repositÃ³rio por seguranÃ§a.  
> O arquivo `aws_credentials.json` foi usado localmente para leitura via Spark no notebook `00-Configuracao`.

---






<br></br>
# ğŸ§­ Etapas do Projeto


## 1. ConfiguraÃ§Ã£o Inicial ğŸ› ï¸
O projeto foi construÃ­do com base na integraÃ§Ã£o entre `AWS` e `Databricks`, com o objetivo de aprofundar o uso dessas duas tecnologias.
<br>
Durante o processo de conexÃ£o do Databricks ao AWS S3, percebi que nÃ£o seria seguro deixar as chaves de acesso expostas no cÃ³digo, jÃ¡ que qualquer usuÃ¡rio poderia visualizÃ¡-las.
<br>
Pensando em boas prÃ¡ticas e seguranÃ§a, esta etapa inicial foi dedicada Ã  configuraÃ§Ã£o do ambiente, incluindo a criaÃ§Ã£o de um arquivo
`aws_credentials.json`, que permite a conexÃ£o com a AWS de forma segura e controlada.

#### 1.A - ConfiguraÃ§Ã£o das chaves de acesso | [00-Configuracao ğŸ“](notebooks/00-Configuracao.ipynb)
- CriaÃ§Ã£o da conta na AWS
- CriaÃ§Ã£o do bucket S3 com as camadas bronze, silver e gold
- CriaÃ§Ã£o de credenciais IAM e geraÃ§Ã£o das chaves de acesso
- Armazenamento seguro das credenciais em um arquivo `.json`
- ConfiguraÃ§Ã£o do acesso ao S3 no Databricks usando `spark.conf`


<br></br>


## 2. ExtraÃ§Ã£o dos Dados e CatalogaÃ§Ã£o Inicial da camada Bronze ğŸ“¦ğŸ¥‰
Nesta etapa, realizamos a extraÃ§Ã£o dos dados brutos (raw) da fonte original (`Kaggle`), com armazenamento direto na camada Bronze do nosso Data Lake no S3 (`mvp-brasileirao-2024`).
<br>
Em seguida, iniciamos o processo de catalogaÃ§Ã£o para entender a estrutura dos dados recebidos e decidir quais colunas seriam aproveitadas nas prÃ³ximas etapas do pipeline.

#### 2.A - ExtraÃ§Ã£o dos Dados e armazenamento no AWS S3 |  [Arquivos da camada Bronze](https://github.com/Cavalheiro93/mvp-brasileirao-data-engineering/tree/main/data/bronze)
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-bronze-arquivos-raw.jpg)

#### 2.B  CatÃ¡logo da Camada Bronze | [01-Catalogo de Dados no Metastore do Databricks BronzeğŸ“](notebooks/01-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Bronze.ipynb)  
- CriaÃ§Ã£o do catÃ¡logo no Metastore do Databricks com os dados brutos (raw) armazenados na camada Bronze
- Registro do nome dos campos, tipo e respectivas descriÃ§Ãµes de cada um deles
- AdiÃ§Ã£o de atributos informativos: valores mÃ­nimos e mÃ¡ximos, total de registros, registros nulos e registros distintos
- Detalhamento da fonte dos dados: link da origem, nome do arquivo original e nome utilizado no Databricks/S3


<br></br>


## 3. Processamento da Camada Silver: Limpeza, TransformaÃ§Ã£o e CatalogaÃ§Ã£o dos Dados ğŸ§¹ğŸ¥ˆ
ApÃ³s a extraÃ§Ã£o e armazenamento dos dados brutos na camada Bronze, esta etapa Ã© dedicada Ã  preparaÃ§Ã£o dos dados para consumo analÃ­tico.
<br>
Realizamos a limpeza, padronizaÃ§Ã£o e transformaÃ§Ã£o de cada uma das tabelas originais, tratando problemas como colunas irrelevantes, formataÃ§Ãµes inconsistentes e divergÃªncias nas datas das partidas.
<br>
Ao final do processo, os dados tratados sÃ£o armazenados na camada Silver do Data Lake no formato Delta e Parquet e, em seguida, registrados no Metastore do Databricks com o caminho S3, possibilitando consultas diretas via SQL.


#### 3.A - Dados Transformados e armazenados em Parquet e Delta na camada Silver no AWS S3 | [Arquivos da camada Silver](https://github.com/Cavalheiro93/mvp-brasileirao-data-engineering/tree/main/data/silver)
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-silver-pastas-parquet-delta.jpg)


#### 3B. IngestÃ£o Bronze - Todas as Partidas | [02A-Ingestao-Bronze-TodasPartidasğŸ“](notebooks/02A-Ingestao-Bronze-TodasPartidas.ipynb)  
- Leitura do arquivo `BrasilSerieA_2024_TodasPartidas.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes (ex: colunas com odds de apostas)
- Filtragem da temporada **2024** apenas.
- RemoÃ§Ã£o de linhas duplicadas e com dados ausentes crÃ­ticos
- CriaÃ§Ã£o das colunas:
  - `Match_ID`: identificador Ãºnico da partida
  - `Trimestre`: com base na data da partida
  - `Turno`: para indicar se Ã© 1Âº ou 2Âº turno do campeonato
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3C. IngestÃ£o Bronze - ClassificaÃ§Ã£o Final | [02B-Ingestao-Bronze-ClassificacaoğŸ“](notebooks/02B-Ingestao-Bronze-Classificacao.ipynb)
- Leitura do arquivo `BrasilSerieA_2024_ClassificacaoFinal.csv` na Bronze
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3D. IngestÃ£o Bronze - EstatÃ­sticas por Jogador e Partida | [02C-Ingestao-Bronze-EstatisticaJogadorPorPartidağŸ“](notebooks/02C-Ingestao-Bronze-EstatisticaJogadorPorPartida.ipynb)
- Leitura do arquivo `BrasilSerieA_2024_EstatisticaJogadorPorPartida.csv` na Bronze
- RemoÃ§Ã£o de colunas irrelevantes ou redundantes
- RenomeaÃ§Ã£o de colunas para padronizaÃ§Ã£o
- CriaÃ§Ã£o de um dicionÃ¡rio de clubes, para padronizar os nomes
- CorreÃ§Ã£o no Tipo de Dado de algumas colunas
- âš ï¸ Ajuste na base de dados por falta de informaÃ§Ã£o
- Armazenamento do DataFrame tratado na **camada Silver** no formato Parquet


#### 3E. CorreÃ§Ã£o de Datas e Partidas Ausentes | [02D-Correcao-Datas-e-Partidas-AusentesğŸ“](notebooks/02D-Correcao-Datas-e-Partidas-Ausentes.ipynb)
- IdentificaÃ§Ã£o de divergÃªncias entre datas de partidas nos arquivos de estatÃ­sticas e todas as partidas
- IdentificaÃ§Ã£o do padrÃ£o onde as datas diferentes das partidas sÃ£o de -1 dia
- CorreÃ§Ã£o das datas incorretas com base no dataset validado (fbref.com)
- Armazenamento do DataFrame final corrigido na camada Silver no formato Parquet


#### 3F. CatÃ¡logo da Camada Silver | [03-Catalogo de Dados no Metastore do Databricks SilverğŸ“](notebooks/03-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Silver.ipynb)
- CriaÃ§Ã£o do Database especÃ­fico para os dados tratados (camada Silver)
- ConversÃ£o dos arquivos tratados de .parquet para o formato Delta
- Registro das tabelas da camada Silver no Metastore com o caminho no S3


<br></br>


## 4. TransformaÃ§Ãµes AnalÃ­ticas para Camada Gold e CatÃ¡logo de Dados ğŸ§ªğŸ¥‡
Nesta etapa, consolidamos os dados tratados da camada Silver e criamos as primeiras tabelas analÃ­ticas do nosso **Data Warehouse**.

A Camada Gold serÃ¡ responsÃ¡vel por organizar os dados de forma otimizada para consumo analÃ­tico, atravÃ©s de **Data Marts** modelados por clube e por jogador, alÃ©m de outras tabelas complementares.

O CatÃ¡logo de dados da Camada Gold foram registrados no **Metastore do Databricks** com caminho no S3, permitindo consultas SQL e integraÃ§Ã£o com ferramentas analÃ­ticas.
![VisualizaÃ§Ã£o da Camada Bronze no S3](images/AWS-S3/bucket-s3-camada-gold-pastas-arquivos-finais.jpg)

#### 4A. TransformaÃ§Ã£o por Clube | [04A-Transformacao-Gold-Mart-Clubes](notebooks/04A-Transformacao-Gold-Mart-Clubes.ipynb)  
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- CriaÃ§Ã£o de Views TemporÃ¡rias
- CriaÃ§Ã£o de mÃ©tricas de normalizaÃ§Ã£o
- Criando pesos para cada uma dessas mÃ©tricas
- Criando um Score beseado na soma dessas mÃ©tricas de normalizaÃ§Ã£o, para o resultado final do desempenho do clube


#### 4B. TransformaÃ§Ã£o por Jogador | [04B-Transformacao-Gold-Mart-Jogadores](notebooks/04B-Transformacao-Gold-Mart-Jogadores.ipynb)
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- CriaÃ§Ã£o de Views TemporÃ¡rias
- CriaÃ§Ã£o de mÃ©tricas de normalizaÃ§Ã£o
- Criando pesos para cada uma dessas mÃ©tricas
- Criando um Score beseado na soma dessas mÃ©tricas de normalizaÃ§Ã£o, para o resultado final do desempenho do Jogador


#### 4C. InformaÃ§Ãµes Complementares dos Jogadores | [04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb](notebooks/04C-Transformacao-Gold-Mart_Info_Jogadores.ipynb)
- Leitura dos Arquivos no DBFS (Para diminuir o custo da AWS S3)
- Mapeamento das posiÃ§Ãµes dos Jogadores em cada partida
- Ranqueamento das duas posiÃ§Ãµes mais jogadas por cada jogador
- CriaÃ§Ã£o de colunas de PosiÃ§Ã£o Principal e ImprovisaÃ§Ã£o

#### 4D. CatÃ¡logo da Camada Gold | [05-Catalogo de Dados no Metastore do Databricks Gold](notebooks/05-Catalogo%20de%20Dados%20no%20Metastore%20do%20Databricks%20Gold.ipynb)
- CriaÃ§Ã£o do Database especÃ­fico para os dados tratados (camada Gold)
- Registro das tabelas da camada Gold no Metastore com o caminho no S3
- AdiÃ§Ã£o de atributos informativos: valores mÃ­nimos e mÃ¡ximos, total de registros, registros nulos e registros distintos
- Detalhamento da fonte dos dados: link da origem, nome do arquivo original e nome utilizado no Databricks/S3
